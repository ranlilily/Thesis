{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import music21 as m21\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib as imp\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import a02_transformer\n",
    "imp.reload(a02_transformer)\n",
    "import a00_funs_make_symbol_seqs as fmseq\n",
    "from a01_melody_preprocessor import MelodyPreprocessor\n",
    "from a02_transformer import TransformerModel\n",
    "from a04_melody_generator import MelodyGenerator\n",
    "import a03_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定參數，並且載入資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/lib/python3.10/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "18523\n",
      "187\n"
     ]
    }
   ],
   "source": [
    "## Parameters for Data Preprocessing\n",
    "time_signature = '4/4'\n",
    "beats_per_measure=4\n",
    "step_duration = 0.25  # 0.25 = a 1/16 note \n",
    "acceptable_durations = np.arange(0.25, 8.1, 0.25) \n",
    "\n",
    "\n",
    "## Import Data and Prepare batches\n",
    "songs = m21.corpus.search('bach', fileExtensions='xml')\n",
    "melodies = fmseq.make_melody_symbol_sequences(songs, time_signature, \n",
    "                                              acceptable_durations)\n",
    "preprocessor = MelodyPreprocessor(melodies)\n",
    "training_dataset = preprocessor.create_training_dataset()\n",
    "training_batches = DataLoader(training_dataset, shuffle=True,\n",
    "                              batch_size=128)\n",
    "\n",
    "print(preprocessor.vocab_size)\n",
    "print(preprocessor.data_size)\n",
    "print(preprocessor.seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "儲存成 pickle 檔案，讓 Data 不要每次都重載一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存儲 preprocessor 物件\n",
    "with open('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "# 存儲 training_dataset\n",
    "with open('training_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(training_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 preprocessor 物件\n",
    "with open('preprocessor.pkl', 'rb') as f:\n",
    "    preprocessor2 = pickle.load(f)\n",
    "\n",
    "# 載入 training_dataset\n",
    "with open('training_dataset.pkl', 'rb') as f:\n",
    "    training_dataset2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_padding_mask(seq, pad_token=0):\n",
    "    return (seq == pad_token)\n",
    "\n",
    "def look_ahead_mask(dim):\n",
    "    return nn.Transformer.generate_square_subsequent_mask(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(num_pos, d_model):\n",
    "    position = torch.arange(num_pos).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                         (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    angles = position * div_term\n",
    "    pos_encoding = torch.zeros(num_pos, d_model)\n",
    "    pos_encoding[:, 0::2] = torch.sin(angles)\n",
    "    pos_encoding[:, 1::2] = torch.cos(angles)\n",
    "    return pos_encoding.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dropout, dim_feedforward, vocab_size_padding,\n",
    "                 num_encoder_layers, num_decoder_layers, device):\n",
    "        super(TransformerModel, self).__init__()  \n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size_padding, d_model).to(device)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, \n",
    "                                                        dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_encoder_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, \n",
    "                                                        dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer = nn.Linear(d_model, vocab_size_padding)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_padding_mask = key_padding_mask(src).to(self.device)\n",
    "        tgt_padding_mask = key_padding_mask(tgt).to(self.device)\n",
    "        tgt_mask = look_ahead_mask(tgt.size(-1)).to(self.device)  \n",
    "        scale_factor = torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32, device=self.device))\n",
    "\n",
    "        x = self.embedding(src) \n",
    "        x *= scale_factor\n",
    "        x += position_encoding(src.size(-1), self.d_model).to(self.device)\n",
    "        x = self.dropout(x)\n",
    "        enc_output = self.encoder(x, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "        y = self.embedding(tgt)\n",
    "        y *= scale_factor\n",
    "        y += position_encoding(tgt.size(-1), self.d_model).to(self.device)\n",
    "        y = self.dropout(y)\n",
    "        dec_output = self.decoder(y, enc_output, tgt_mask=tgt_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask)\n",
    "        output = self.final_layer(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Model Specification and Training\n",
    "vocab_size_padding = preprocessor.vocab_size + 1\n",
    "model = TransformerModel(d_model=128, nhead=2, dim_feedforward=128, dropout=0.1, \n",
    "                         vocab_size_padding=vocab_size_padding, \n",
    "                         num_encoder_layers=6, num_decoder_layers=6, device=device)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 0.3114804326460279\n",
      "Epoch 2/3, Average Loss: 0.2570605012877234\n",
      "Epoch 3/3, Average Loss: 0.21700749911110975\n"
     ]
    }
   ],
   "source": [
    "# epochs = 200\n",
    "# save_interval = 20\n",
    "epochs = 3\n",
    "save_interval =1\n",
    "save_dir= \"/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/teacher_transformercode_0226/epoch\"\n",
    "start_sequence = ['C4-1.0', 'G4-1.0', 'E4-1.0', 'C4-1.0']\n",
    "\n",
    "losses = []\n",
    "epoch_times = [] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    average_loss = a03_train.train_each_step(training_batches, model, \n",
    "                                             criterion, optimizer, device)\n",
    "    losses.append(average_loss)  # 將當前 epoch 的 loss 加入到列表中\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_times.append(epoch_duration)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss},Duration: {epoch_duration} seconds')\n",
    "\n",
    "    if epoch > 0 and (epoch + 1) % save_interval == 0:\n",
    "        melody_generator = MelodyGenerator(model, preprocessor.tokenizer, device)\n",
    "        new_melody = melody_generator.generate(start_sequence, preprocessor.tokenizer)\n",
    "        np.savetxt(f\"{save_dir}/{epoch + 1}.txt\", new_melody, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 loss 和每個 epoch 的執行時間寫入同一個文件中\n",
    "with open(f\"{save_dir}/loss_and_epoch_times.txt\", \"w\") as file:\n",
    "    for epoch, (loss, duration) in enumerate(zip(losses, epoch_times), 1):\n",
    "        file.write(f'Epoch {epoch}, Average Loss: {loss}, Duration: {duration} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存模型\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他紀錄：\n",
    "- CPU : one epoch for  about 30 mins\n",
    "- CPU : 3 epoch for 104m\n",
    "- 可以嘗試不同的起始值的影響"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS_Pytorch_Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
