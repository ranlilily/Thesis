{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 22:45:16.790733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import music21 as m21\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib as imp\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import a02_transformer\n",
    "imp.reload(a02_transformer)\n",
    "import a00_funs_make_symbol_seqs as fmseq\n",
    "imp.reload(fmseq)\n",
    "from a01_melody_preprocessor import MelodyPreprocessor\n",
    "from a02_transformer import TransformerModel\n",
    "from a04_melody_generator import MelodyGenerator\n",
    "import a03_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for Data Preprocessing\n",
    "time_signature = '4/4'\n",
    "beats_per_measure=4\n",
    "step_duration = 0.25  # 0.25 = a 1/16 note \n",
    "acceptable_durations = np.arange(0.25, 8.1, 0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kern_Dataset_Path = \"/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/Thesis/German_Folk/deutschl/erk\"\n",
    "Save_Path = \"/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/Thesis/German_Folk/dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入資料集 （如果已經載入過，可以直接使用 pickle 檔）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data and Prepare batches\n",
    "songs = fmseq.load_songs_in_kern(Kern_Dataset_Path)\n",
    "melodies = fmseq.make_melody_symbol_sequences(songs, time_signature, acceptable_durations, Save_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "25762\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "preprocessor = MelodyPreprocessor(melodies)\n",
    "training_dataset = preprocessor.create_training_dataset()\n",
    "training_batches = DataLoader(training_dataset, shuffle=True,\n",
    "                              batch_size=128)\n",
    "\n",
    "print(preprocessor.vocab_size)\n",
    "print(preprocessor.data_size)\n",
    "print(preprocessor.seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "儲存成 pickle 檔案，讓 Data 不要每次都重載一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存儲 preprocessor 物件\n",
    "with open('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "# 存儲 training_dataset\n",
    "with open('training_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(training_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之後要使用，載入 pickle 檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 載入 preprocessor 物件\n",
    "# with open('preprocessor.pkl', 'rb') as f:\n",
    "#     preprocessor = pickle.load(f)\n",
    "\n",
    "# # 載入 training_dataset\n",
    "# with open('training_dataset.pkl', 'rb') as f:\n",
    "#     training_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_batches = DataLoader(training_dataset, shuffle=True,\n",
    "#                               batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def key_padding_mask(seq, pad_token=0):\n",
    "#     return (seq == pad_token)\n",
    "\n",
    "# def look_ahead_mask(dim):\n",
    "#     return nn.Transformer.generate_square_subsequent_mask(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述方法會出現 warning:\n",
    "UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_padding_mask(seq, pad_token=0):\n",
    "    # 轉換為布林類型\n",
    "    return (seq == pad_token).bool()\n",
    "\n",
    "def look_ahead_mask(dim):  #使下三角變為 ０（from ChatGPT 有點怪）\n",
    "    # 使用相同類型的遮罩\n",
    "    mask = torch.triu(torch.ones(dim, dim, dtype=torch.bool), diagonal=1)\n",
    "    return mask  # 將布林張量取反 ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Mask:\n",
      "tensor([[False, False,  True,  True],\n",
      "        [False,  True,  True,  True],\n",
      "        [False, False, False,  True]])\n",
      "\n",
      "Look Ahead Mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "def test_masks():\n",
    "    # 測試 key_padding_mask\n",
    "    seq = torch.tensor([[1, 2, 0, 0], [3, 0, 0, 0], [4, 5, 6, 0]])\n",
    "    pad_token = 0\n",
    "    padding_mask = key_padding_mask(seq, pad_token)\n",
    "    print(\"Padding Mask:\")\n",
    "    print(padding_mask)\n",
    "\n",
    "    # 測試 look_ahead_mask\n",
    "    dim = 4\n",
    "    ahead_mask = look_ahead_mask(dim)\n",
    "    print(\"\\nLook Ahead Mask:\")\n",
    "    print(ahead_mask)\n",
    "\n",
    "test_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def padding_mask_2(seq, pad_idx):\n",
    "#     return (seq != pad_idx).unsqueeze(-2)   # [B, 1, L]\n",
    "\n",
    "# def sequence_mask_2(seq):\n",
    "#     batch_size, seq_len = seq.size()\n",
    "#     mask = 1- torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),diagonal=1)\n",
    "#     mask = (mask!=0).unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]\n",
    "#     return mask\n",
    "\n",
    "# def test():\n",
    "#     # 以最简化的形式测试Transformer的两种mask\n",
    "#     seq = torch.LongTensor([[1,2,0]]) # batch_size=1, seq_len=3，padding_idx=0\n",
    "#     embedding = torch.nn.Embedding(num_embeddings=3, embedding_dim=10, padding_idx=0)\n",
    "#     query, key = embedding(seq), embedding(seq)\n",
    "#     scores = torch.matmul(query, key.transpose(-2, -1))  #最后得到的token之间相互的分数\n",
    "\n",
    "#     mask_p = padding_mask_2(seq, 0)\n",
    "#     mask_s = sequence_mask_2(seq)\n",
    "#     mask_decoder = mask_p & mask_s # 结合 padding mask 和 sequence mask\n",
    "#     print(mask_p)\n",
    "#     print(mask_s)\n",
    "#     print(mask_decoder)\n",
    "\n",
    "#     scores_encoder = scores.masked_fill(mask_p==0, -1e9) # 对于scores，在mask==0的位置填充\n",
    "#     scores_decoder = scores.masked_fill(mask_decoder==0, -1e9)\n",
    "#     print(scores_encoder)\n",
    "#     print(scores_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(num_pos, d_model):\n",
    "    position = torch.arange(num_pos).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                         (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    angles = position * div_term\n",
    "    pos_encoding = torch.zeros(num_pos, d_model)\n",
    "    pos_encoding[:, 0::2] = torch.sin(angles)\n",
    "    pos_encoding[:, 1::2] = torch.cos(angles)\n",
    "    return pos_encoding.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dropout, dim_feedforward, vocab_size_padding,\n",
    "                 num_encoder_layers, num_decoder_layers, device):\n",
    "        super(TransformerModel, self).__init__()  \n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size_padding, d_model).to(device)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, \n",
    "                                                        dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_encoder_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, \n",
    "                                                        dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_layer = nn.Linear(d_model, vocab_size_padding)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_padding_mask = key_padding_mask(src).to(self.device)\n",
    "        tgt_padding_mask = key_padding_mask(tgt).to(self.device)\n",
    "        tgt_mask = look_ahead_mask(tgt.size(-1)).to(self.device)  \n",
    "        scale_factor = torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32, device=self.device))\n",
    "\n",
    "        x = self.embedding(src) \n",
    "        x *= scale_factor\n",
    "        x += position_encoding(src.size(-1), self.d_model).to(self.device)\n",
    "        x = self.dropout(x)\n",
    "        enc_output = self.encoder(x, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "        y = self.embedding(tgt)\n",
    "        y *= scale_factor\n",
    "        y += position_encoding(tgt.size(-1), self.d_model).to(self.device)\n",
    "        y = self.dropout(y)\n",
    "        dec_output = self.decoder(y, enc_output, tgt_mask=tgt_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask)\n",
    "        output = self.final_layer(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Model Specification and Training\n",
    "vocab_size_padding = preprocessor.vocab_size + 1\n",
    "model = TransformerModel(d_model=128, nhead=2, dim_feedforward=128, dropout=0.1, \n",
    "                         vocab_size_padding=vocab_size_padding, \n",
    "                         num_encoder_layers=6, num_decoder_layers=6, device=device)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     12\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m     average_loss \u001b[38;5;241m=\u001b[39m \u001b[43ma03_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_each_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(average_loss)  \u001b[38;5;66;03m# 將當前 epoch 的 loss 加入到列表中\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/python_ve/MS_Pytorch_Thesis/Thesis/German_Folk/a03_train.py:28\u001b[0m, in \u001b[0;36mtrain_each_step\u001b[0;34m(training_batches, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt_batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Shift targets for loss calculation\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/python_ve/MS_Pytorch_Thesis/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/python_ve/MS_Pytorch_Thesis/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epochs = 200\n",
    "# save_interval = 20\n",
    "epochs = 3\n",
    "save_interval = 1\n",
    "save_dir= \"/Users/ranli/Documents/python_ve/MS_Pytorch_Thesis/Thesis/German_Folk/epoch_train\"\n",
    "start_sequence = ['C4-1.0', 'G4-1.0', 'E4-1.0', 'C4-1.0']\n",
    "\n",
    "losses = []\n",
    "epoch_times = [] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    average_loss = a03_train.train_each_step(training_batches, model, \n",
    "                                             criterion, optimizer, device)\n",
    "    losses.append(average_loss)  # 將當前 epoch 的 loss 加入到列表中\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_times.append(epoch_duration)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss},Duration: {epoch_duration} seconds')\n",
    "\n",
    "    if epoch > 0 and (epoch + 1) % save_interval == 0:\n",
    "        melody_generator = MelodyGenerator(model, preprocessor.tokenizer, device)\n",
    "        new_melody = melody_generator.generate(start_sequence, preprocessor.tokenizer)\n",
    "        np.savetxt(f\"{save_dir}/{epoch + 1}.txt\", new_melody, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 loss 和每個 epoch 的執行時間寫入同一個文件中\n",
    "with open(f\"{save_dir}/loss_and_epoch_times.txt\", \"w\") as file:\n",
    "    for epoch, (loss, duration) in enumerate(zip(losses, epoch_times), 1):\n",
    "        file.write(f'Epoch {epoch}, Average Loss: {loss}, Duration: {duration} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generation\n",
    "# start_sequence = [\"C4-2.0\", \"G4-2.0\", \"E4-2.0\", \"D4-1.0\", \"C4-1.0\"]\n",
    "# start_sequence = [\"C4-2.0\", \"F4-2.0\", \"A4-1.0\", \"D5-0.5\", \"C5-0.5\"]\n",
    "# melody_generator = MelodyGenerator(model, preprocessor.tokenizer, device)\n",
    "# new_melody = melody_generator.generate(start_sequence, preprocessor.tokenizer)\n",
    "# print(f\"Generated melody: {new_melody}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "torch.save(optimizer.state_dict(), 'optimizer_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load model\n",
    "# model = TransformerModel(d_model=128, nhead=2, dim_feedforward=128, dropout=0.1, \n",
    "#                          vocab_size_padding=vocab_size_padding, \n",
    "#                          num_encoder_layers=2, num_decoder_layers=2, device=device)\n",
    "# model.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "# optimizer.load_state_dict(torch.load('optimizer_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 loss 和每個 epoch 的執行時間文件\n",
    "losses = []\n",
    "epoch_times = []\n",
    "with open(f\"{save_dir}/loss_and_epoch_times.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.startswith('Epoch'):\n",
    "            parts = line.strip().split(', ')\n",
    "            loss = float(parts[1].split(': ')[1])\n",
    "            losses.append(loss)\n",
    "            duration = float(parts[2].split(': ')[1].split()[0])\n",
    "            epoch_times.append(duration)\n",
    "\n",
    "# 生成 epochs\n",
    "epochs = range(1, len(losses) + 1)\n",
    "\n",
    "# 繪製 loss 曲線\n",
    "plt.plot(epochs, losses, 'y', label='Training loss', linewidth=2.0)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated_melody =('C4-2.0', 'F4-2.0', 'A4-1.0', 'D5-0.5', 'C5-0.5', 'B4-2.0', 'A4-2.0', 'R-1.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-0.5', 'D5-0.5', 'D5-2.0', 'C5-0.5', 'D5-0.5', 'D5-2.0', 'C5-2.0', 'R-1.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'B4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-0.5', 'D5-0.5', 'C5-0.5', 'B4-2.0', 'R-1.0', 'D5-0.5', 'D5-2.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'B4-1.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'B4-1.0', 'A4-1.0', 'A4-2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import music21 as m21\n",
    "\n",
    "# def melody_symbols_to_midi(melody_symbols, output_file):\n",
    "#     stream = m21.stream.Stream()\n",
    "#     for symbol in melody_symbols:\n",
    "#         if symbol.startswith('R'):\n",
    "#             rest_duration = float(symbol.split('-')[1])\n",
    "#             rest = m21.note.Rest(quarterLength=rest_duration)\n",
    "#             stream.append(rest)\n",
    "#         else:\n",
    "#             pitch_name, duration = symbol.split('-')\n",
    "#             note = m21.note.Note(pitch_name, quarterLength=float(duration))\n",
    "#             stream.append(note)\n",
    "#     stream.write('midi', fp=output_file)\n",
    "\n",
    "# # 使用示例\n",
    "# Generated_melody = ['C4-2.0', 'F4-2.0', 'A4-1.0', 'D5-0.5', 'C5-0.5', 'B4-2.0', 'A4-2.0', 'R-1.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-0.5', 'D5-0.5', 'D5-2.0', 'C5-0.5', 'D5-0.5', 'D5-2.0', 'C5-2.0', 'R-1.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'B4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-0.5', 'D5-0.5', 'C5-0.5', 'B4-2.0', 'R-1.0', 'D5-0.5', 'D5-2.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'B4-1.0', 'C5-1.0', 'C5-1.0', 'B4-1.0', 'B4-1.0', 'A4-1.0', 'A4-2.0']\n",
    "# melody_symbols_to_midi(Generated_melody, 'generated_melody.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated_melody_2 =  ['C4-1.0', 'G4-1.0', 'E4-0.5', 'D4-0.5', 'C4-2.0', 'G4-1.0', 'A4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'A4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-1.0', 'B4-1.0', 'A4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-1.0', 'B4-1.0', 'A4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-1.0', 'B4-1.0', 'A4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0', 'B4-1.0', 'C5-1.0', 'G4-1.0', 'E4-1.0', 'A4-1.0', 'A4-1.0', 'G4-1.0', 'G4-1.0', 'G4-1.0', 'A4-1.0']\n",
    "# melody_symbols_to_midi(Generated_melody_2, 'generated_melody_2.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他紀錄：\n",
    "- CPU : one epoch for  about 30 mins\n",
    "- CPU : 3 epoch for 104m\n",
    "- 可以嘗試不同的起始值的影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True, False]]])\n",
      "tensor([[[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]]])\n",
      "tensor([[[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True, False]]])\n",
      "tensor([[[ 6.8092e+00, -9.4008e-01, -1.0000e+09],\n",
      "         [-9.4008e-01,  6.5163e+00, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[ 6.8092e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-9.4008e-01,  6.5163e+00, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# def padding_mask_2(seq, pad_idx):\n",
    "#     return (seq != pad_idx).unsqueeze(-2)   # [B, 1, L]\n",
    "\n",
    "# def sequence_mask_2(seq):\n",
    "#     batch_size, seq_len = seq.size()\n",
    "#     mask = 1- torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),diagonal=1)\n",
    "#     mask = (mask!=0).unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]\n",
    "#     return mask\n",
    "\n",
    "# def test():\n",
    "#     # 以最简化的形式测试Transformer的两种mask\n",
    "#     seq = torch.LongTensor([[1,2,0]]) # batch_size=1, seq_len=3，padding_idx=0\n",
    "#     embedding = torch.nn.Embedding(num_embeddings=3, embedding_dim=10, padding_idx=0)\n",
    "#     query, key = embedding(seq), embedding(seq)\n",
    "#     scores = torch.matmul(query, key.transpose(-2, -1))  #最后得到的token之间相互的分数\n",
    "\n",
    "#     mask_p = padding_mask_2(seq, 0)\n",
    "#     mask_s = sequence_mask_2(seq)\n",
    "#     mask_decoder = mask_p & mask_s # 结合 padding mask 和 sequence mask\n",
    "#     print(mask_p)\n",
    "#     print(mask_s)\n",
    "#     print(mask_decoder)\n",
    "\n",
    "#     scores_encoder = scores.masked_fill(mask_p==0, -1e9) # 对于scores，在mask==0的位置填充\n",
    "#     scores_decoder = scores.masked_fill(mask_decoder==0, -1e9)\n",
    "#     print(scores_encoder)\n",
    "#     print(scores_decoder)\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS_Pytorch_Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
